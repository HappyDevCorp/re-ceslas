# A Comprehensive Evaluation between Small Language Models and Large Language Models with Advantage towards SLMs - CESLAS

## List of Benchmarks

| âœ… | Benchmark    | Description                                                                 | Paper | Link |
|----|-------------|-----------------------------------------------------------------------------|-----------| --- |
| âœ… | MMLU         | 57-subject exam for factual knowledge and reasoning.                        | [ğŸ“„ Paper](https://arxiv.org/abs/2009.03300) | [ğŸ”— GitHub](https://github.com/hendrycks/test) |
| âœ… | ARC          | Science questions needing reasoning beyond simple facts.                    | [ğŸ“„ Paper](https://arxiv.org/abs/1803.05457) | [ğŸ”— HuggingFace](https://huggingface.co/datasets/ai2_arc) |
| âœ… | BoolQ        | Yes/no questions over short paragraphs.                                     | [ğŸ“„ Paper](https://arxiv.org/abs/1905.10044) | [ğŸ“¦ HuggingFace](https://huggingface.co/datasets/boolq) |
| âŒ | Natural Questions | Google search queries + long answers from Wikipedia.                 | [ğŸ“„ Paper](https://research.google/pubs/pub47761/) | [ğŸ”— Website](https://ai.google.com/research/NaturalQuestions) |
| âœ… | LAMBADA      | Predict final word of a passage needing long-term context.                  | [ğŸ“„ Paper](https://arxiv.org/abs/1606.06031) | [ğŸ“¦ Dataset](https://huggingface.co/datasets/lambada) |
| âœ… | HellaSwag    | Contextual sentence completion (adversarial).                               | [ğŸ“„ Paper](https://arxiv.org/abs/1905.07830) | [ğŸ”— GitHub](https://github.com/rowanz/hellaswag) |
| âœ… | MultiNLI     | Natural language inference across genres.                                   | [ğŸ“„ Paper](https://arxiv.org/abs/1704.05426) | [ğŸ“¦ Dataset](https://huggingface.co/datasets/multi_nli) |
| âœ… | SuperGLUE    | Harder variant of GLUE benchmark.                                           | [ğŸ“„ Paper](https://arxiv.org/abs/1905.00537) | [ğŸ”— Website](https://super.gluebenchmark.com/) |
| âŒ | TriviaQA     | QA benchmark with evidence retrieval.                                       | [ğŸ“„ Paper](https://arxiv.org/abs/1705.03551) | [ğŸ”— GitHub](https://github.com/mandarjoshi90/triviaqa) |
| âœ… | WinoGrande   | Commonsense reasoning with pronoun disambiguation.                          | [ğŸ“„ Paper](https://arxiv.org/abs/1907.10641) | [ğŸ“¦ Dataset](https://huggingface.co/datasets/winogrande) |
| âœ… | SciQ         | Science multiple-choice QA.                                                 | [ğŸ“„ Paper](https://arxiv.org/abs/1707.06209) | [ğŸ“¦ Dataset](https://huggingface.co/datasets/sciq) |
| âœ… | GSM8K       | Grade-school math for multi-step reasoning.                                 | [ğŸ“„ Paper](https://arxiv.org/abs/2110.14168) | [ğŸ“¦ Dataset](https://huggingface.co/datasets/gsm8k) |
| âœ… | DROP        | Discrete reasoning over paragraphs (addition, comparison, etc.).             | [ğŸ“„ Paper](https://arxiv.org/abs/1903.00161) | [ğŸ“¦ Dataset](https://huggingface.co/datasets/drop) |
| âœ… | CRASS       | Counterfactual "what-if" scenario reasoning.                                 | [ğŸ“„ Paper](https://arxiv.org/abs/2112.11941) | [ğŸ”— GitHub](https://github.com/apergo-ai/CRASS-data-set) |
| âŒ | RACE        | English exam reading comprehension.                                           | [ğŸ“„ Paper](https://arxiv.org/abs/1704.04683) | [ğŸ“¦ Dataset](https://www.cs.cmu.edu/~glai1/data/race/) |
| âœ… | BBH         | BIG-Bench Hard â€” multi-step and complex reasoning.                           | [ğŸ“„ Paper](https://arxiv.org/abs/2210.09261) | [ğŸ”— GitHub](https://github.com/suzgunmirac/BIG-Bench-Hard) |
| âœ… | AGIEval     | Standardized test QA (SAT, GRE, LSAT, etc.).                                  | [ğŸ“„ Paper](https://arxiv.org/abs/2304.06364) | [ğŸ”— GitHub](https://github.com/ruixiangcui/AGIEval) |
| âœ… | BoolQ       | 15,000+ real yes/no questions from Google paired with Wikipedia passages      | [ğŸ“„ Paper](https://arxiv.org/abs/1905.10044) | [ğŸ“¦ Dataset](https://huggingface.co/datasets/boolq) |
| âœ… | MT-bench    | Evaluates chat assistants on multi-turn coherence | [ğŸ“„ Paper](https://arxiv.org/abs/2306.05685) | [ğŸ“¦ Dataset](https://huggingface.co/datasets/lmsys/mt_bench_human_judgments) |
| âœ… | QuAC | 14,000 dialogues and 100,000 Q&A pairs | [ğŸ“„ Paper](https://arxiv.org/abs/1808.07036) | [ğŸ“¦ Dataset](https://quac.ai/) |
| âŒ | ACI-BENCH | Doctor-patient conversations + clinical notes | [ğŸ“„ Paper](https://arxiv.org/abs/2306.02022) | [ğŸ”— GitHub](https://github.com/wyim/aci-bench) |
| âœ… | MS-MARCO | Real web queries with human answers | [ğŸ“„ Paper](https://arxiv.org/abs/1611.09268) | [ğŸ“¦ Dataset](https://huggingface.co/datasets/ms_marco) |
| âŒ | QMSum | Meeting summarization via queries | [ğŸ“„ Paper](https://arxiv.org/abs/2104.05938) | [ğŸ”— GitHub](https://github.com/Yale-LILY/QMSum) |
| âœ… | PIQA | Tests physical reasoning through QA | [ğŸ“„ Paper](https://arxiv.org/abs/1911.11641) | [ğŸ”— GitHub](https://github.com/ybisk/ybisk.github.io/tree/master/piqa) |
| âŒ | ToxiGen | Toxic/benign statements on minorities | [ğŸ“„ Paper](https://arxiv.org/abs/2203.09509) | [ğŸ”— GitHub](https://github.com/microsoft/TOXIGEN/tree/main), [ğŸ“¦ Dataset](https://huggingface.co/datasets/skg/toxigen-data) |
| âœ… | HHH (Helpfulness, Honesty, Harmlessness) | Evaluates model alignment to ethical standards | [ğŸ“„ Paper](https://arxiv.org/abs/2112.00861) | [ğŸ”— GitHub](https://github.com/anthropics/hh-rlhf)|
| âœ… | TruthfulQA | Measures truthfulness in response to misleading queries | [ğŸ“„ Paper](https://arxiv.org/abs/2109.07958v2) | [ğŸ”— GitHub](https://github.com/sylinrl/TruthfulQA) |
| âŒ | RAI (Responsible AI) | Evaluates safety risks like IP leakage in chats | [ğŸ“„ Paper](https://arxiv.org/abs/2310.17750) | N/A |
| âœ… | CodeXGLUE | Tests code understanding, generation, completion | [ğŸ“„ Paper](https://arxiv.org/abs/2102.04664) | [ğŸ”— GitHub](https://github.com/microsoft/CodeXGLUE) |
| âœ… | HumanEval | Programming tasks to test code generation | [ğŸ“„ Paper](https://arxiv.org/abs/2107.03374) | [ğŸ”— GitHub](https://github.com/openai/human-eval) |
| âœ… | MBPP | 1,000 simple Python tasks for beginners | [ğŸ“„ Paper](https://arxiv.org/abs/2108.07732) | [ğŸ“¦ Dataset](https://huggingface.co/datasets/mbpp) |


